import os
import logging
import time
import asyncio
import random
import datetime
from typing import Dict, Any, Optional, Union, List, Tuple
import functools

from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.constants import ParseMode
from telegram.ext import ContextTypes
from telegram.error import TimedOut, NetworkError

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª–∏ –ø—Ä–æ–µ–∫—Ç–∞
import token_storage
from utils import process_token_data, format_enhanced_message, format_number
from error_helpers import handle_exception
from api_cache import get_token_info_from_api
from http_client import http_client
from notifications import add_growth_notification
from token_monitor_strategy import token_monitor_strategy
from handlers.auth_middleware import user_required, admin_required
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç Telegram
_telegram_context = None

def set_telegram_context(context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç Telegram.
    
    Args:
        context: –ö–æ–Ω—Ç–µ–∫—Å—Ç Telegram
    """
    global _telegram_context
    _telegram_context = context

def get_telegram_context() -> Optional[ContextTypes.DEFAULT_TYPE]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç Telegram.
    
    Returns:
        –ö–æ–Ω—Ç–µ–∫—Å—Ç Telegram –∏–ª–∏ None
    """
    return _telegram_context

@handle_exception(log_msg="–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–∫–µ–Ω–µ", notify_user=True)
async def get_token_info(
    query: str, 
    chat_id: Optional[int], 
    message_id: Optional[int] = None, 
    context: Optional[ContextTypes.DEFAULT_TYPE] = None,
    force_refresh: bool = False  # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä force_refresh
) -> Optional[Dict[str, Any]]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–µ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ."""
    logger.info(f"–ó–∞–ø—Ä–æ—Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–∫–µ–Ω–µ: {query}, force_refresh: {force_refresh}")
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–∫–µ–Ω–µ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    stored_data = token_storage.get_token_data(query)
    
    token_data = None
    token_info = None
    need_fresh_data = force_refresh  # –ï—Å–ª–∏ force_refresh=True, –≤—Å–µ–≥–¥–∞ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ
    
    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –µ—Å—Ç—å –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
    if stored_data and 'token_info' in stored_data and not force_refresh:
        last_update_time = stored_data.get('last_update_time', 0)
        current_time = time.time()
        
        # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª—è–ª–∏—Å—å –º–µ–Ω–µ–µ 1 –º–∏–Ω—É—Ç—ã –Ω–∞–∑–∞–¥, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
        if current_time - last_update_time < 60:  # 60 —Å–µ–∫—É–Ω–¥ = 1 –º–∏–Ω—É—Ç–∞
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–∫–µ–Ω–µ {query}")
            token_info = stored_data['token_info']
            # –¢–∞–∫–∂–µ –ø–æ–ª—É—á–∞–µ–º raw_api_data, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
            token_data = stored_data.get('raw_api_data')
        else:
            logger.info(f"–î–∞–Ω–Ω—ã–µ –æ —Ç–æ–∫–µ–Ω–µ {query} —É—Å—Ç–∞—Ä–µ–ª–∏, –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å–≤–µ–∂–∏–µ")
            need_fresh_data = True
    else:
        if force_refresh:
            logger.info(f"–ó–∞–ø—Ä–æ—à–µ–Ω–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–∫–µ–Ω–µ {query}")
        else:
            logger.info(f"–î–∞–Ω–Ω—ã–µ –æ —Ç–æ–∫–µ–Ω–µ {query} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ, –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å API")
        need_fresh_data = True
    
    # –ï—Å–ª–∏ –Ω—É–∂–Ω—ã —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ, –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –∏—Ö –∏–∑ API
    if need_fresh_data:
        api_data = await fetch_token_api_data(query)
        if not api_data:
            return await handle_api_error(query, chat_id, message_id, context)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ API
        token_data, token_info = process_api_data(api_data)
        if not token_info:
            return await handle_api_error(query, chat_id, message_id, context)
    
    # –ü–æ–ª—É—á–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    initial_data = get_initial_data(query, token_info)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —Ä–æ—Å—Ç–µ
    growth_notification = check_growth_notification(query, token_info, initial_data)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
    message = format_enhanced_message(token_info, initial_data)
    
    # –ë–æ–ª—å—à–µ –Ω–µ —Å–æ–∑–¥–∞—ë–º –∫–Ω–æ–ø–∫—É Refresh –≤ –æ–¥–∏–Ω–æ—á–Ω–æ–º –æ–∫–Ω–µ
    reply_markup = None
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ chat_id
    if context and chat_id:
        await send_or_update_message(
            query, chat_id, message_id, context, message, reply_markup, token_data, token_info, initial_data
        )
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —Ä–æ—Å—Ç–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if growth_notification and context and chat_id:
            query = growth_notification['query']
            current_multiplier = growth_notification['current_multiplier']
            token_info = growth_notification['token_info']
            ticker = token_info['ticker']
            market_cap = token_info['market_cap']
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ notifications.py
            add_growth_notification(chat_id, ticker, current_multiplier, market_cap)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞–ª–µ—Ä—Ç
            token_storage.update_token_field(query, 'last_alert_multiplier', current_multiplier)
    
    return token_info

@handle_exception(log_msg="–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö API")
async def fetch_token_api_data(query: str) -> Optional[Dict[str, Any]]:
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–∫–µ–Ω–µ –∏–∑ API —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è."""
    from config import DEXSCREENER_API_URL
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    api_data = get_token_info_from_api(query)
    
    if api_data:
        logger.info(f"–ü–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –∏–∑ API –¥–ª—è {query}")
        return api_data
    
    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ API –¥–ª—è {query}")
    return None

@handle_exception(log_msg="–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö API")
def process_api_data(api_data: Dict[str, Any]) -> Tuple[Dict[str, Any], Optional[Dict[str, Any]]]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –æ—Ç API."""
    pairs = api_data.get('pairs', [])
    
    if not pairs:
        return {}, None
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π
    token_data = pairs[0]
    raw_api_data = token_data
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏
    token_info = process_token_data(token_data)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤ token_info
    token_info['price_usd'] = token_data.get('priceUsd')  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∫—É—Ä—Å
    
    if 'txns' in token_data:
        token_info['txns'] = token_data.get('txns', {})
        
    if 'labels' in token_data:
        token_info['labels'] = token_data.get('labels', [])
        
    if 'audit' in token_data:
        token_info['audit'] = token_data.get('audit', {})
        
    if 'pyr' in token_data:
        token_info['pyr'] = token_data.get('pyr')
    
    return raw_api_data, token_info

@handle_exception(log_msg="–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—à–∏–±–∫–∏ API")
async def handle_api_error(
    query: str, 
    chat_id: Optional[int], 
    message_id: Optional[int], 
    context: Optional[ContextTypes.DEFAULT_TYPE]
) -> Optional[Dict[str, Any]]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—à–∏–±–∫–∏ API –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ö—Ä–∞–Ω—è—â–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å."""
    stored_data = token_storage.get_token_data(query)
    
    if stored_data and 'token_info' in stored_data:
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ–∫–µ–Ω–∞ {query}")
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å context –∏ chat_id, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
        if context and chat_id:
            await context.bot.send_message(
                chat_id=chat_id,
                text=f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ–∫–µ–Ω–∞ '{query}'. –ü–æ–∫–∞–∑—ã–≤–∞—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ."
            )
        
        return stored_data.get('token_info')
    
    if context and chat_id:
        if message_id:
            await context.bot.edit_message_text(
                chat_id=chat_id,
                message_id=message_id,
                text=f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–µ '{query}'."
            )
        else:
            await context.bot.send_message(
                chat_id=chat_id,
                text=f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–µ '{query}'."
            )
    
    return None

@handle_exception(log_msg="–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
def get_initial_data(query: str, token_info: Dict[str, Any]) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç –Ω–∞—á–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–∫–µ–Ω–µ."""
    stored_data = token_storage.get_token_data(query)
    
    if stored_data and 'initial_data' in stored_data:
        return stored_data.get('initial_data')
    
    # –ï—Å–ª–∏ –Ω–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ
    current_time = datetime.datetime.now().strftime("%H:%M:%S")
    initial_data = {
        'time': current_time,
        'market_cap': token_info['market_cap'],
        'raw_market_cap': token_info['raw_market_cap'],
        'price_usd': token_info.get('price_usd', 0)
    }
    
    logger.info(f"–ù–æ–≤—ã–π —Ç–æ–∫–µ–Ω {query} –¥–æ–±–∞–≤–ª–µ–Ω —Å –Ω–∞—á–∞–ª—å–Ω—ã–º Market Cap: {token_info['market_cap']}")
    
    return initial_data

@handle_exception(log_msg="–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ä–æ—Å—Ç–∞")
def check_growth_notification(
    query: str,
    token_info: Dict[str, Any], 
    initial_data: Optional[Dict[str, Any]]
) -> Optional[Dict[str, Any]]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —Ä–æ—Å—Ç–µ."""
    if not initial_data or 'raw_market_cap' not in initial_data or not initial_data['raw_market_cap']:
        return None
    
    if 'raw_market_cap' not in token_info or not token_info['raw_market_cap']:
        return None
    
    initial_mcap = initial_data['raw_market_cap']
    current_mcap = token_info['raw_market_cap']
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å
    if initial_mcap <= 0:
        return None
    
    multiplier = current_mcap / initial_mcap
    current_multiplier = int(multiplier)
    
    # –ï—Å–ª–∏ –º–Ω–æ–∂–∏—Ç–µ–ª—å < 2, –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
    if current_multiplier < 2:
        return None
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –∞–ª–µ—Ä—Ç –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –º–Ω–æ–∂–∏—Ç–µ–ª—è
    stored_data = token_storage.get_token_data(query)
    if not stored_data:
        return None
    
    last_alert_multiplier = stored_data.get('last_alert_multiplier', 1)
    
    # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –º–Ω–æ–∂–∏—Ç–µ–ª—å <= –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∞–ª–µ—Ä—Ç, –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
    if current_multiplier <= last_alert_multiplier:
        return None
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    growth_percent = (multiplier - 1) * 100
    
    if not token_monitor_strategy.should_notify_growth(query, growth_percent):
        logger.info(f"–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —Ä–æ—Å—Ç–µ —Ç–æ–∫–µ–Ω–∞ {query} –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏")
        return None
        
    return {
        'query': query,
        'current_multiplier': current_multiplier,
        'token_info': token_info
    }

@handle_exception(log_msg="Error sending/updating message")
async def send_or_update_message(
    query: str,
    chat_id: int,
    message_id: Optional[int],
    context: ContextTypes.DEFAULT_TYPE,
    message: str,
    reply_markup: InlineKeyboardMarkup,
    token_data: Dict[str, Any],
    token_info: Dict[str, Any],
    initial_data: Dict[str, Any]
) -> None:
    """Sends new message or updates existing one."""
    if message_id:
        try:
            await context.bot.edit_message_text(
                chat_id=chat_id,
                message_id=message_id,
                text=message,
                parse_mode=ParseMode.MARKDOWN,
                reply_markup=reply_markup,
                disable_web_page_preview=True
            )
            
            # Update data in storage with same message_id
            token_data_to_store = {
                'last_update_time': time.time(),
                'message_id': message_id,  # Keep existing message_id
                'chat_id': chat_id,
                'initial_data': initial_data,
                'token_info': token_info,
                'last_alert_multiplier': token_storage.get_token_data(query).get('last_alert_multiplier', 1) if token_storage.get_token_data(query) else 1,
                'added_time': token_storage.get_token_data(query).get('added_time', time.time()) if token_storage.get_token_data(query) else time.time(),
                'raw_api_data': token_data,
                'current_price_usd': token_info.get('price_usd', 0)
            }
            
            token_storage.store_token_data(query, token_data_to_store)
            logger.info(f"Updated message for token {query} (message_id: {message_id})")
            
        except Exception as e:
            if "Message is not modified" not in str(e):
                logger.error(f"Error updating message: {e}")
    else:
        try:
            # Send to admin first
            sent_msg = await context.bot.send_message(
                chat_id=chat_id,
                text=message,
                parse_mode=ParseMode.MARKDOWN,
                reply_markup=reply_markup,
                disable_web_page_preview=True
            )
            
            # NEW: Send to ALL recipients (admin + authorized users)
            from handlers.auth_middleware import get_user_db
            from config import CONTROL_ADMIN_IDS
            user_db = get_user_db()
            all_users = user_db.get_all_users()
            active_users = [user for user in all_users if user['is_active']]
            
            # Create list of all recipients
            recipients = []
            
            # Add active users (excluding admin to avoid duplicate - admin already got message above)
            for user in active_users:
                if user['user_id'] not in CONTROL_ADMIN_IDS:  # Skip admin (already sent above)
                    recipients.append(user)
            
            # Send to all recipients
            for user in recipients:
                try:
                    await context.bot.send_message(
                        chat_id=user['user_id'],
                        text=message,
                        parse_mode=ParseMode.MARKDOWN,
                        disable_web_page_preview=True
                    )
                    logger.info(f"New token info sent to user {user['user_id']}")
                except Exception as e:
                    logger.error(f"Error sending new token to user {user['user_id']}: {e}")
            
            # Save token data with admin's message_id
            token_data_to_store = {
                'last_update_time': time.time(),
                'message_id': sent_msg.message_id,
                'chat_id': sent_msg.chat_id,
                'initial_data': initial_data,
                'token_info': token_info,
                'last_alert_multiplier': 1,
                'added_time': time.time(),
                'raw_api_data': token_data,
                'current_price_usd': token_info.get('price_usd', 0)
            }
            
            token_storage.store_token_data(query, token_data_to_store)
            logger.info(f"Sent new message for token {query} (message_id: {sent_msg.message_id})")
            
        except Exception as e:
            logger.error(f"Error sending new message: {e}")

@handle_exception(log_msg="–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ —Ä–æ—Å—Ç–µ")
async def send_growth_notification_handler(
    growth_data: Dict[str, Any],
    chat_id: int,
    message_id: int,
    context: ContextTypes.DEFAULT_TYPE
) -> None:
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ —Ä–æ—Å—Ç–µ —Ç–æ–∫–µ–Ω–∞.
    –ü–æ—Å–ª–µ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è —Å—Ä–∞–∑—É —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π ATH, –µ—Å–ª–∏ –æ–Ω –≤—ã—à–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ.
    """
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    query              = growth_data['query']
    current_multiplier = growth_data['current_multiplier']
    token_info         = growth_data['token_info']
    ticker             = token_info['ticker']
    market_cap_str     = token_info['market_cap']
    raw_mcap           = token_info['raw_market_cap']  # —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

    # 1. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    await add_growth_notification(
        chat_id=chat_id,
        ticker=ticker,
        multiplier=current_multiplier,
        market_cap=market_cap_str,
        reply_to_message_id=message_id
    )

    # 2. –û–±–Ω–æ–≤–ª—è–µ–º –≤ –ë–î –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –º–Ω–æ–∂–∏—Ç–µ–ª—å
    token_storage.update_token_field(
        query=query,
        field_name='last_alert_multiplier',
        value=current_multiplier
    )

    # 3. –§–∏–∫—Å–∏—Ä—É–µ–º –Ω–æ–≤—ã–π ATH, –µ—Å–ª–∏ –æ–Ω –≤—ã—à–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ
    #    –í–Ω—É—Ç—Ä–∏ update_token_ath –µ—Å—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ —É—Å–ª–æ–≤–∏–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏
    token_storage.update_token_ath(query=query, new_mcap=raw_mcap)
    logger.info(f"ATH –¥–ª—è —Ç–æ–∫–µ–Ω–∞ {query} –æ–±–Ω–æ–≤–ª—ë–Ω –¥–æ {raw_mcap}")

@handle_exception(log_msg="–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞–¥—Ä–µ—Å–∞ —Ç–æ–∫–µ–Ω–∞")
async def process_token_address(address: str, chat_id: int, context: ContextTypes.DEFAULT_TYPE) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∞–¥—Ä–µ—Å —Ç–æ–∫–µ–Ω–∞, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –æ—Ç –≤–Ω–µ—à–Ω–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–¥—Ä–µ—Å–∞ —Ç–æ–∫–µ–Ω–∞: {address}")
    
    try:
        # –£–≤–µ–¥–æ–º–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ –Ω–∞—á–∞–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        msg = await context.bot.send_message(
            chat_id=chat_id,
            text=f"–ü–æ–ª—É—á–µ–Ω –Ω–æ–≤—ã–π –∫–æ–Ω—Ç—Ä–∞–∫—Ç: {address}\n–ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–µ..."
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –¥–∞–Ω–Ω—ã–µ –æ–± —ç—Ç–æ–º —Ç–æ–∫–µ–Ω–µ
        stored_data = token_storage.get_token_data(address)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è—Ö
        set_telegram_context(context)
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–µ
        result = await get_token_info(address, chat_id, stored_data.get('message_id') if stored_data else None, context)
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–∞ {address}: {'—É—Å–ø–µ—à–Ω–æ' if result else '–Ω–µ —É–¥–∞–ª–æ—Å—å'}")
        
        # –£–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –ø–æ–∏—Å–∫–µ
        try:
            await msg.delete()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è –æ –ø–æ–∏—Å–∫–µ: {e}")
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞–¥—Ä–µ—Å–∞ —Ç–æ–∫–µ–Ω–∞: {e}")
        import traceback
        logger.error(traceback.format_exc())
        try:
            await context.bot.send_message(
                chat_id=chat_id,
                text=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ {address}: {str(e)}"
            )
        except:
            pass

@handle_exception(log_msg="–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–∞—Ä–∫–µ—Ç –∫–∞–ø–∞")
async def check_market_cap(
    query: str,
    chat_id: Optional[int] = None,
    message_id: Optional[int] = None,
    context: Optional[ContextTypes.DEFAULT_TYPE] = None,
    check_growth: bool = True
) -> Optional[Dict[str, Any]]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –º–∞—Ä–∫–µ—Ç –∫–∞–ø —Ç–æ–∫–µ–Ω–∞ –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–Ω–æ–∂–∏—Ç–µ–ª–∏ —Ä–æ—Å—Ç–∞.
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–∫–µ–Ω–µ –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        stored_data = token_storage.get_token_data(query)
        if not stored_data:
            logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–∫–µ–Ω–µ {query} –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
            return None
        
        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º API —á–µ—Ä–µ–∑ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
        api_data = get_token_info_from_api(query)
        
        if not api_data:
            logger.warning(f"API –Ω–µ –≤–µ—Ä–Ω—É–ª–æ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–æ–∫–µ–Ω–∞ {query}")
            return None
        
        pairs = api_data.get('pairs', [])
        
        if not pairs:
            logger.warning(f"API –Ω–µ –≤–µ—Ä–Ω—É–ª–æ –¥–∞–Ω–Ω—ã–µ –æ –ø–∞—Ä–∞—Ö –¥–ª—è —Ç–æ–∫–µ–Ω–∞ {query}")
            return None
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        token_data = pairs[0]
        
        # –ü–æ–ª—É—á–∞–µ–º –∏ –æ–±–Ω–æ–≤–ª—è–µ–º market cap
        market_cap = token_data.get('fdv')
        raw_market_cap = market_cap
        market_cap_formatted = format_number(market_cap)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª—è –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        if 'token_info' in stored_data:
            stored_data['token_info']['market_cap'] = market_cap_formatted
            stored_data['token_info']['raw_market_cap'] = raw_market_cap
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            stored_data['last_update_time'] = time.time()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            token_storage.store_token_data(query, stored_data)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º ATH, –µ—Å–ª–∏ —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤—ã—à–µ
            if raw_market_cap:
                token_storage.update_token_ath(query, raw_market_cap)
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
            result = {
                'market_cap': market_cap_formatted,
                'raw_market_cap': raw_market_cap
            }
            
            # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–æ—Å—Ç, –¥–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            if check_growth:
                send_notification = False
                current_multiplier = 1
                multiplier = 1  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º multiplier –∑–Ω–∞—á–µ–Ω–∏–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                
                initial_data = stored_data.get('initial_data', {})
                initial_mcap = initial_data.get('raw_market_cap', 0)
                
                if initial_mcap and initial_mcap > 0 and raw_market_cap:
                    # –í—ã—á–∏—Å–ª—è–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å
                    multiplier = raw_market_cap / initial_mcap
                    current_multiplier = int(multiplier)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –∞–ª–µ—Ä—Ç –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –º–Ω–æ–∂–∏—Ç–µ–ª—è
                    last_alert_multiplier = stored_data.get('last_alert_multiplier', 1)
                    
                    # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –º–Ω–æ–∂–∏—Ç–µ–ª—å >= 2 –∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∞–ª–µ—Ä—Ç
                    if current_multiplier >= 2 and current_multiplier > last_alert_multiplier:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
                        growth_percent = (multiplier - 1) * 100
                        send_notification = token_monitor_strategy.should_notify_growth(query, growth_percent)
                        
                        if send_notification:
                            logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–æ–≤—ã–π –º–Ω–æ–∂–∏—Ç–µ–ª—å –¥–ª—è —Ç–æ–∫–µ–Ω–∞ {query}: x{current_multiplier} (–ø—Ä–µ–¥—ã–¥—É—â–∏–π: x{last_alert_multiplier})")
                
                result.update({
                    'multiplier': multiplier,  # –¢–µ–ø–µ—Ä—å multiplier –≤—Å–µ–≥–¥–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω
                    'current_multiplier': current_multiplier,
                    'send_notification': send_notification
                })
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–æ–∫–µ–Ω–∞ –≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            token_monitor_strategy.update_token_category(query, stored_data)
            
            return result
        else:
            logger.warning(f"–í —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ—Ç –ø–æ–ª—è token_info –¥–ª—è —Ç–æ–∫–µ–Ω–∞ {query}")
            return None
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–∞—Ä–∫–µ—Ç –∫–∞–ø–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–∞ {query}: {e}")
        return None

@handle_exception(log_msg="–û—à–∏–±–∫–∞ –≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ —Ç–æ–∫–µ–Ω–æ–≤")
async def monitor_token_market_caps(context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –º–∞—Ä–∫–µ—Ç –∫–∞–ø —Ç–æ–∫–µ–Ω–æ–≤ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏.
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        active_tokens = token_storage.get_active_tokens()
        
        if not active_tokens:
            logger.info("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –º–∞—Ä–∫–µ—Ç –∫–∞–ø–∞")
            return
            
        logger.info(f"–ó–∞–ø—É—â–µ–Ω –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–∞—Ä–∫–µ—Ç –∫–∞–ø–∞, –≤—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(active_tokens)}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–µ–π—á–∞—Å
        tokens_to_check = token_monitor_strategy.get_tokens_for_check(active_tokens)
        
        if not tokens_to_check:
            logger.info("–ù–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ —Ç–µ–∫—É—â–µ–º —Ü–∏–∫–ª–µ")
            return
            
        logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ {len(tokens_to_check)} —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ {len(active_tokens)}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è—Ö
        set_telegram_context(context)
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –æ–±–Ω–æ–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ç –∫–∞–ø –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–æ—Å—Ç
        for query in tokens_to_check:
            try:
                token_data = active_tokens[query]
                chat_id = token_data.get('chat_id')
                message_id = token_data.get('message_id')  # ID —Å–æ–æ–±—â–µ–Ω–∏—è —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–æ–∫–µ–Ω–µ
                
                if not chat_id or not message_id:
                    logger.warning(f"–ù–µ—Ç chat_id –∏–ª–∏ message_id –¥–ª—è —Ç–æ–∫–µ–Ω–∞ {query}")
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –æ –º–∞—Ä–∫–µ—Ç –∫–∞–ø–µ
                result = await check_market_cap(query, chat_id, message_id, context, check_growth=True)
                
                if result:
                    logger.debug(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞ {query}: MC={result.get('market_cap')}, Multiplier={result.get('multiplier', 1)}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–æ—Å—Ç –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if result.get('send_notification', False):
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —Ä–æ—Å—Ç–µ –∫–∞–∫ –æ—Ç–≤–µ—Ç –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Ç–æ–∫–µ–Ω–µ
                        current_multiplier = result.get('current_multiplier', 1)
                        
                        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
                        token_info = token_data.get('token_info', {})
                        ticker = token_info.get('ticker', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        market_cap = result.get('market_cap', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –∞–ª–µ—Ä—Ç –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –º–Ω–æ–∂–∏—Ç–µ–ª—è
                        last_alert_multiplier = token_data.get('last_alert_multiplier', 1)
                        
                        if current_multiplier > last_alert_multiplier:
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —Ä–æ—Å—Ç–µ –∫–∞–∫ –æ—Ç–≤–µ—Ç –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                            await add_growth_notification(chat_id, ticker, current_multiplier, market_cap, message_id)
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞–ª–µ—Ä—Ç
                            token_storage.update_token_field(query, 'last_alert_multiplier', current_multiplier)
                            logger.info(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —Ä–æ—Å—Ç–µ —Ç–æ–∫–µ–Ω–∞ {ticker} –¥–æ x{current_multiplier}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –ø–∞—É–∑—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ API
                await asyncio.sleep(random.uniform(1.0, 2.5))
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ —Ç–æ–∫–µ–Ω–∞ {query}: {e}")
                continue
                
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –º–∞—Ä–∫–µ—Ç –∫–∞–ø–∞: {e}")
        import traceback
        logger.error(traceback.format_exc())



@handle_exception(log_msg="Error sending token statistics")
async def send_token_stats(context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    Sends token statistics for the last 12 hours to ALL authorized users.
    This function runs on schedule.
    """
    logger.info("=== STARTING TOKEN STATISTICS GENERATION ===")
    
    # Get all tokens for analytics
    all_tokens = token_storage.get_all_tokens_for_analytics()
    logger.info(f"Loaded tokens for analysis: {len(all_tokens)}")
    
    if not all_tokens:
        logger.info("No tokens to generate statistics")
        return
    
    # Current time
    current_time = time.time()
    logger.info(f"Current time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Time 12 hours ago
    time_12h_ago = current_time - (12 * 60 * 60)
    logger.info(f"Time 12 hours ago: {datetime.datetime.fromtimestamp(time_12h_ago).strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Statistics counters
    total_tokens = 0
    tokens_1_5x = 0
    tokens_2x = 0
    tokens_5x = 0
    
    # List of tokens for detailed logging
    analyzed_tokens = []
    
    # Check each token
    for query, data in all_tokens.items():
        # Check if token was added within last 12 hours
        added_time = data.get('added_time', 0)
        
        if not added_time:
            logger.info(f"Token {query} has no add time, skipping")
            continue
            
        # Log token add time
        token_added_time = datetime.datetime.fromtimestamp(added_time).strftime('%Y-%m-%d %H:%M:%S')
        logger.info(f"Token {query} added: {token_added_time}")
        
        if added_time < time_12h_ago:
            logger.info(f"Token {query} added more than 12 hours ago, skipping")
            continue
            
        # Get initial market cap
        initial_mcap = 0
        if 'initial_data' in data and 'raw_market_cap' in data['initial_data']:
            initial_mcap = data['initial_data'].get('raw_market_cap', 0)
        
        # Use ATH market cap instead of current
        ath_market_cap = data.get('ath_market_cap', 0)
        
        # Log market caps
        logger.info(f"Token {query} - Initial mcap: {initial_mcap}, ATH mcap: {ath_market_cap}")
        
        # Skip tokens without market cap data
        if not initial_mcap or not ath_market_cap:
            logger.info(f"Token {query} has no market cap data, skipping")
            continue
        
        # Calculate multiplier based on ATH
        multiplier = ath_market_cap / initial_mcap if initial_mcap > 0 else 0
        logger.info(f"Token {query} - Multiplier: {multiplier:.2f}x")
        
        # Update counters - using mutually exclusive categories
        total_tokens += 1
        
        if multiplier >= 5:
            tokens_5x += 1
        elif multiplier >= 2:
            tokens_2x += 1
        elif multiplier >= 1.5:
            tokens_1_5x += 1
        
        # Add to list for detailed logging
        ticker = "Unknown"
        if 'token_info' in data and 'ticker' in data['token_info']:
            ticker = data['token_info']['ticker']
            
        analyzed_tokens.append({
            'query': query,
            'ticker': ticker,
            'added_time': token_added_time,
            'initial_mcap': initial_mcap,
            'ath_mcap': ath_market_cap,
            'multiplier': multiplier
        })
    
    # Log detailed token statistics
    logger.info(f"Analyzed tokens for last 12 hours: {total_tokens}")
    logger.info(f"Tokens with growth 1.5x to <2x: {tokens_1_5x}")
    logger.info(f"Tokens with growth 2x to <5x: {tokens_2x}")
    logger.info(f"Tokens with growth ‚â•5x: {tokens_5x}")
    
    for token in analyzed_tokens:
        logger.info(f"Token {token['ticker']} ({token['query']}): added {token['added_time']}, multiplier {token['multiplier']:.2f}x")
    
    # Generate statistics message
    if total_tokens > 0:
        # Calculate success rate (>=1.5x)
        successful_tokens = tokens_1_5x + tokens_2x + tokens_5x
        hitrate_percent = (successful_tokens / total_tokens) * 100 if total_tokens > 0 else 0
        
        # Determine symbol for success rate visualization
        hitrate_symbol = "üî¥"  # <30%
        if hitrate_percent >= 70:
            hitrate_symbol = "üü£"  # >=70%
        elif hitrate_percent >= 50:
            hitrate_symbol = "üü¢"  # >=50%
        elif hitrate_percent >= 30:
            hitrate_symbol = "üü°"  # >=30%
        
        message = (
            f"Token stats for the last 12 hours:\n"
            f"> Total tokens: {total_tokens}\n"
            f"‚îú 1.5x-2x: {tokens_1_5x}\n"
            f"‚îú 2x-5x: {tokens_2x}\n"
            f"‚îî ‚â•5x: {tokens_5x}\n\n"
            f"Hitrate: {hitrate_percent:.1f}% {hitrate_symbol} (1.5x+)"
        )
        
        # NEW: Get ALL recipients (admin + active users)
        from handlers.auth_middleware import get_user_db
        from config import CONTROL_ADMIN_IDS
        user_db = get_user_db()
        all_users = user_db.get_all_users()
        active_users = [user for user in all_users if user['is_active']]
        
        # Create list of all recipients
        recipients = []
        
        # Add admin (always receives notifications)
        for admin_id in CONTROL_ADMIN_IDS:
            recipients.append({'user_id': admin_id, 'username': 'admin'})
        
        # Add active users (excluding admin duplicate)
        for user in active_users:
            if user['user_id'] not in CONTROL_ADMIN_IDS:  # Avoid duplicate
                recipients.append(user)
        
        if not recipients:
            logger.warning("No recipients to send statistics")
            return
        
        logger.info(f"Sending statistics to {len(recipients)} recipients (admin + active users)")
        
        # Send message to ALL recipients
        success_count = 0
        for user in recipients:
            try:
                logger.info(f"Sending statistics to chat {user['user_id']}...")
                
                # Add retry handling for sending
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        await context.bot.send_message(
                            chat_id=user['user_id'],
                            text=message
                        )
                        logger.info(f"Token statistics successfully sent to chat {user['user_id']}")
                        success_count += 1
                        break
                    except (TimedOut, NetworkError) as e:
                        if attempt < max_retries - 1:
                            logger.warning(f"Timeout sending statistics to chat {user['user_id']} (attempt {attempt+1}/{max_retries}): {e}")
                            await asyncio.sleep(2)
                        else:
                            logger.error(f"Failed to send statistics to chat {user['user_id']} after {max_retries} attempts: {e}")
            except Exception as e:
                logger.error(f"Error sending statistics to chat {user['user_id']}: {e}")
                import traceback
                logger.error(traceback.format_exc())
        
        logger.info(f"Statistics successfully sent to {success_count} out of {len(recipients)} chats")
    else:
        logger.info("No tokens in last 12 hours to generate statistics")
    
    logger.info("=== TOKEN STATISTICS GENERATION COMPLETED ===")

def get_signals_data(contract_address: str) -> Optional[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Å–∏–≥–Ω–∞–ª–∞—Ö –∏–∑ tracker –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.
    
    Args:
        contract_address: –ê–¥—Ä–µ—Å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ —Ç–æ–∫–µ–Ω–∞
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –æ —Å–∏–≥–Ω–∞–ª–∞—Ö –∏–ª–∏ None
    """
    try:
        import json
        import os
        from datetime import datetime
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ tracker –±–∞–∑—ã
        tracker_db_file = 'tokens_tracker_database.json'
        if not os.path.exists(tracker_db_file):
            return None
            
        with open(tracker_db_file, 'r', encoding='utf-8') as f:
            tracker_db = json.load(f)
        
        # –ò—â–µ–º —Ç–æ–∫–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞
        token_data = tracker_db.get(contract_address)
        if not token_data:
            return None
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ –∏ –≤—Ä–µ–º–µ–Ω–∞ —Å–∏–≥–Ω–∞–ª–æ–≤
        channels = token_data.get('channels', [])
        channel_times = token_data.get('channel_times', {})
        first_seen = token_data.get('first_seen', '')
        
        if not channels:
            return None
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        signal_times = []
        channels_with_times = []
        
        for channel in channels:
            if channel in channel_times:
                time_str = channel_times[channel]
                try:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Ä–µ–º—è –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ HH:MM:SS –≤ –ø–æ–ª–Ω—É—é –¥–∞—Ç—É
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç—É –∏–∑ first_seen –∏–ª–∏ —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É
                    if first_seen:
                        # –ï—Å–ª–∏ –µ—Å—Ç—å first_seen, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–∞—Ç—É
                        base_date = datetime.now().strftime('%Y-%m-%d')
                    else:
                        base_date = datetime.now().strftime('%Y-%m-%d')
                    
                    full_datetime = f"{base_date} {time_str}"
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–∞ –≤—Ä–µ–º–µ–Ω–∏
                    datetime.strptime(time_str, '%H:%M:%S')
                    
                    signal_times.append(full_datetime)
                    channels_with_times.append(f"{channel} ({time_str})")
                except ValueError:
                    # –ï—Å–ª–∏ –≤—Ä–µ–º—è –≤ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    signal_times.append(time_str)
                    channels_with_times.append(f"{channel} ({time_str})")
            else:
                channels_with_times.append(channel)
        
        return {
            'total_signals': len(channels),
            'channels_list': ' | '.join(channels_with_times),
            'signal_times': signal_times
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è {contract_address}: {e}")
        return None

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è excel —Å —Ç–æ–∫–µ–Ω–∞–º–∏ 
@handle_exception(log_msg="–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏")
async def generate_analytics_excel(context: ContextTypes.DEFAULT_TYPE, chat_id: int) -> None:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Excel —Ñ–∞–π–ª —Å –ü–û–õ–ù–û–ô –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π –ø–æ –≤—Å–µ–º —Ç–æ–∫–µ–Ω–∞–º (–≤–∫–ª—é—á–∞—è —Å—Ç–∞—Ä—ã–µ)."""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –í–°–ï —Ç–æ–∫–µ–Ω—ã (–≤–∫–ª—é—á–∞—è —Å—Ç–∞—Ä—à–µ 3 –¥–Ω–µ–π)
        all_tokens = token_storage.get_all_tokens_for_analytics()
        
        if not all_tokens:
            await context.bot.send_message(
                chat_id=chat_id,
                text="–ù–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏."
            )
            return
        
        logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –¥–ª—è {len(all_tokens)} —Ç–æ–∫–µ–Ω–æ–≤ (–≤–∫–ª—é—á–∞—è —Å—Ç–∞—Ä—ã–µ)")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è Excel (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É —á—Ç–æ –∏ –≤ generate_excel)
        tokens_data = []
        
        for query, token_data in all_tokens.items():
            try:
                token_info = token_data.get('token_info', {})
                initial_data = token_data.get('initial_data', {})
                raw_api_data = token_data.get('raw_api_data', {})
                
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                def safe_float(value, default=0.0):
                    try:
                        if value is None:
                            return default
                        return float(value)
                    except (ValueError, TypeError):
                        return default
                
                def safe_int(value, default=0):
                    try:
                        if value is None:
                            return default
                        return int(value)
                    except (ValueError, TypeError):
                        return default
                
                # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–µ
                ticker = str(token_info.get('ticker', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'))
                ticker_address = str(token_info.get('ticker_address', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'))
                chain_id = str(token_info.get('chain_id', ''))
                pair_address = str(token_info.get('pair_address', ''))
                
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                current_market_cap = safe_float(token_info.get('raw_market_cap'))
                initial_market_cap = safe_float(initial_data.get('raw_market_cap'))
                ath_market_cap = safe_float(token_data.get('ath_market_cap'))
                                
                # –í—ã—á–∏—Å–ª—è–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª–∏ —Ä–æ—Å—Ç–∞
                ath_multiplier = 1.0
                
                if initial_market_cap > 0 and ath_market_cap > 0:
                    ath_multiplier = round(ath_market_cap / initial_market_cap, 2)
                
                # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                added_time = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                if token_data.get('added_time'):
                    try:
                        added_time = datetime.datetime.fromtimestamp(safe_float(token_data.get('added_time'))).strftime('%Y-%m-%d %H:%M:%S')
                    except:
                        added_time = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                
                ath_time = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                if token_data.get('ath_time'):
                    try:
                        ath_time = datetime.datetime.fromtimestamp(safe_float(token_data.get('ath_time'))).strftime('%Y-%m-%d %H:%M:%S')
                    except:
                        ath_time = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                
                # –û–±—ä–µ–º—ã —Ç–æ—Ä–≥–æ–≤
                volume_5m = str(token_info.get('volume_5m', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'))
                volume_1h = str(token_info.get('volume_1h', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'))
                token_age = str(token_info.get('token_age', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'))
                
                # –î–∞–Ω–Ω—ã–µ –æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è—Ö –∏–∑ API
                txns_data = raw_api_data.get('txns', {}) if raw_api_data else {}
                buys_5m = safe_int(txns_data.get('m5', {}).get('buys', 0) if isinstance(txns_data.get('m5'), dict) else 0)
                sells_5m = safe_int(txns_data.get('m5', {}).get('sells', 0) if isinstance(txns_data.get('m5'), dict) else 0)
                buys_1h = safe_int(txns_data.get('h1', {}).get('buys', 0) if isinstance(txns_data.get('h1'), dict) else 0)
                sells_1h = safe_int(txns_data.get('h1', {}).get('sells', 0) if isinstance(txns_data.get('h1'), dict) else 0)
                buys_24h = safe_int(txns_data.get('h24', {}).get('buys', 0) if isinstance(txns_data.get('h24'), dict) else 0)
                sells_24h = safe_int(txns_data.get('h24', {}).get('sells', 0) if isinstance(txns_data.get('h24'), dict) else 0)
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                dex_id = str(raw_api_data.get('dexId', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')) if raw_api_data else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
                liquidity_usd = safe_float(raw_api_data.get('liquidity', {}).get('usd', 0) if raw_api_data else 0)
                                
                # –°—Å—ã–ª–∫–∏
                dexscreener_link = str(token_info.get('dexscreener_link', ''))
                axiom_link = str(token_info.get('axiom_link', ''))
                
                # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É –¥–ª—è Excel —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º —Ç–∏–ø–æ–≤
                row = {
                    # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    'Ticker': ticker,
                    'DEX': dex_id,
                    'Contract Address': ticker_address,
                    'Pair Address': pair_address,
                    'Chain ID': chain_id,
                    
                    'Token Age': token_age,
                    
                    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    'Added Time': added_time,
                    'ATH Time': ath_time,
                    
                    # –¶–µ–Ω—ã –∏ –º–∞—Ä–∫–µ—Ç –∫–∞–ø
                    
                    'Initial Market Cap': format_number(initial_market_cap),
                    'ATH Market Cap': format_number(ath_market_cap),
                    'ATH Multiplier': f"{ath_multiplier}",
                    
                    'Current Market Cap': format_number(current_market_cap),
                                        
                    # –õ–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å
                    'Liquidity USD': format_number(liquidity_usd),
                                        
                    # –°–ª—É–∂–µ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    'Last Update': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö –∏ –≤–µ–±-—Å–∞–π—Ç–∞—Ö
                websites = []
                socials = []
                
                if isinstance(token_info.get('websites'), list):
                    websites = [str(site.get('url', '')) for site in token_info['websites'] if site.get('url')]
                
                if isinstance(token_info.get('socials'), list):
                    socials = [f"{str(social.get('type', ''))}:{str(social.get('url', ''))}" 
                              for social in token_info['socials'] if social.get('url')]
                
                row['Websites'] = ' | '.join(websites[:3]) if websites else ''
                row['Socials'] = ' | '.join(socials[:3]) if socials else ''
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ API
                if raw_api_data:
                    row['FDV'] = safe_float(raw_api_data.get('fdv', 0))
                    row['Price Native'] = safe_float(raw_api_data.get('priceNative', 0))
                    
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã
                    price_change = raw_api_data.get('priceChange', {})
                    if isinstance(price_change, dict):
                        row['Price Change 5m'] = safe_float(price_change.get('m5', 0))
                        row['Price Change 1h'] = safe_float(price_change.get('h1', 0))
                        row['Price Change 6h'] = safe_float(price_change.get('h6', 0))
                        row['Price Change 24h'] = safe_float(price_change.get('h24', 0))
                    else:
                        row['Price Change 5m'] = 0
                        row['Price Change 1h'] = 0
                        row['Price Change 6h'] = 0
                        row['Price Change 24h'] = 0
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Å–∏–≥–Ω–∞–ª–∞—Ö –∏–∑ tracker –±–∞–∑—ã
                from token_service import get_signals_data
                signals_data = get_signals_data(ticker_address)
                if signals_data:
                    row['–û–±—â–µ–µ –∫–æ–ª-–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤'] = signals_data['total_signals']
                    row['–ö–∞–Ω–∞–ª—ã'] = signals_data['channels_list']
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
                    for i, signal_time in enumerate(signals_data['signal_times'], 1):
                        row[f'{i} —Å–∏–≥–Ω–∞–ª'] = signal_time
                else:
                    row['–û–±—â–µ–µ –∫–æ–ª-–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤'] = 0
                    row['–ö–∞–Ω–∞–ª—ã'] = ''
                
                tokens_data.append(row)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–æ–∫–µ–Ω–∞ {query} –¥–ª—è –ø–æ–ª–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {e}")
                
                # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∑–∞–ø–∏—Å—å —Å –æ—à–∏–±–∫–æ–π
                tokens_data.append({
                    'Ticker': str(token_data.get('token_info', {}).get('ticker', '–û—à–∏–±–∫–∞')),
                    'Error': str(e)
                })
        
        if not tokens_data:
            await context.bot.send_message(
                chat_id=chat_id,
                text="–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –ø–æ–ª–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏."
            )
            return
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º Excel —Ñ–∞–π–ª
        import pandas as pd
        df = pd.DataFrame(tokens_data)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ ATH –º–Ω–æ–∂–∏—Ç–µ–ª—é (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
        if 'ATH Multiplier' in df.columns:
            try:
                df['ATH_Multiplier_Sort'] = df['ATH Multiplier'].astype(str).str.replace('x', '').astype(float)
                df = df.sort_values('ATH_Multiplier_Sort', ascending=False)
                df = df.drop('ATH_Multiplier_Sort', axis=1)
            except Exception as sort_e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–µ –ø–æ–ª–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {sort_e}")
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'full_analytics_{timestamp}.xlsx'
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Excel —Ñ–∞–π–ª–∞
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Full Analytics')
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä–µ–∫—Ç –ª–∏—Å—Ç–∞ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            worksheet = writer.sheets['Full Analytics']
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —à–∏—Ä–∏–Ω—É —Å—Ç–æ–ª–±—Ü–æ–≤
            for idx, col in enumerate(df.columns):
                try:
                    max_len = max(
                        df[col].astype(str).map(len).max() if len(df) > 0 else 0,
                        len(str(col))
                    )
                    
                    col_width = min(max_len + 2, 50)
                    
                    if idx < 26:
                        col_letter = chr(65 + idx)
                    else:
                        col_letter = chr(65 + idx // 26 - 1) + chr(65 + idx % 26)
                        
                    worksheet.column_dimensions[col_letter].width = col_width
                except Exception as col_e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ —à–∏—Ä–∏–Ω—ã —Å—Ç–æ–ª–±—Ü–∞ {idx}: {col_e}")
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∞–π–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        try:
            with open(filename, 'rb') as excel_file:
                await context.bot.send_document(
                    chat_id=chat_id, 
                    document=excel_file, 
                    caption=f"üìä Let's analyze!!! ({len(tokens_data)} tokens)"
                )
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.remove(filename)
            logger.info(f"–ü–æ–ª–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ —Å {len(tokens_data)} —Ç–æ–∫–µ–Ω–∞–º–∏ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Ñ–∞–π–ª–∞ –ø–æ–ª–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {e}")
            await context.bot.send_message(
                chat_id=chat_id,
                text="–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–∞–π–ª –ø–æ–ª–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
            )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {e}")
        import traceback
        logger.error(traceback.format_exc())
        await context.bot.send_message(
            chat_id=chat_id,
            text="–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        )

@handle_exception(log_msg="–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
async def start_token_monitoring_system(telegram_context):
    """–ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–æ–º."""
    try:
        # –ò–º–ø–æ—Ä—Ç—ã
        from task_scheduler import scheduler, TaskPriority
        import datetime
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞—á
        scheduler.start()
        logger.info("–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞—á –∑–∞–ø—É—â–µ–Ω")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        scheduler.schedule_task(
            "token_monitor", 
            monitor_token_market_caps, 
            delay=5,  # —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥ –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞
            interval=30,  # –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
            priority=TaskPriority.HIGH,
            context=telegram_context
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ—Ç–ø—Ä–∞–≤–∫—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        morning_time = datetime.time(8, 0, 0)  # 08:00:00
        evening_time = datetime.time(20, 0, 0)  # 20:00:00
        
        now = datetime.datetime.now().time()
        
        # –í—ã—á–∏—Å–ª—è–µ–º, —Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞ —É—Ç—Ä–æ–º
        morning_seconds = (
            (datetime.datetime.combine(datetime.date.today(), morning_time) - 
            datetime.datetime.combine(datetime.date.today(), now)).total_seconds()
        )
        if morning_seconds < 0:
            morning_seconds += 24 * 60 * 60  # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—Ç–∫–∏
            
        # –í—ã—á–∏—Å–ª—è–µ–º, —Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞ –≤–µ—á–µ—Ä–æ–º
        evening_seconds = (
            (datetime.datetime.combine(datetime.date.today(), evening_time) - 
            datetime.datetime.combine(datetime.date.today(), now)).total_seconds()
        )
        if evening_seconds < 0:
            evening_seconds += 24 * 60 * 60  # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—Ç–∫–∏
        
        # –ü–ª–∞–Ω–∏—Ä—É–µ–º —É—Ç—Ä–µ–Ω–Ω—é—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        scheduler.schedule_task(
            "stats_morning", 
            send_token_stats, 
            delay=morning_seconds,  # —á–µ—Ä–µ–∑ –≤—ã—á–∏—Å–ª–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è
            interval=24 * 60 * 60,  # –∫–∞–∂–¥—ã–µ 24 —á–∞—Å–∞
            priority=TaskPriority.NORMAL,
            context=telegram_context
        )
        
        # –ü–ª–∞–Ω–∏—Ä—É–µ–º –≤–µ—á–µ—Ä–Ω—é—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        scheduler.schedule_task(
            "stats_evening", 
            send_token_stats, 
            delay=evening_seconds,  # —á–µ—Ä–µ–∑ –≤—ã—á–∏—Å–ª–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è
            interval=24 * 60 * 60,  # –∫–∞–∂–¥—ã–µ 24 —á–∞—Å–∞
            priority=TaskPriority.NORMAL,
            context=telegram_context
        )
        
        logger.info("–°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞–ø—É—â–µ–Ω–∞")
        return True
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
        return False
    
# ========== –ù–û–í–´–ï –§–£–ù–ö–¶–ò–ò –ë–ê–¢–ß–ò–ù–ì–ê ==========
# token_service.py (–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É —Ñ–∞–π–ª—É)

import logging
import asyncio
import time
from typing import Dict, Any, Optional, List, Tuple
from enum import Enum

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥—É–ª–µ–π
from task_scheduler import scheduler, TaskPriority
from api_cache import (
    fetch_tokens_batch, 
    process_batch_requests, 
    split_into_batches,
    validate_token_addresses
)
import token_storage

logger = logging.getLogger(__name__)

# ========== –ù–û–í–´–ï –ö–õ–ê–°–°–´ –î–õ–Ø –ü–†–ò–û–†–ò–¢–ï–¢–û–í ==========

class TokenPriority(Enum):
    """–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    HIGH = "high"      # –¢–æ–∫–µ–Ω—ã —Å –Ω–µ–¥–∞–≤–Ω–∏–º —Ä–æ—Å—Ç–æ–º - –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
    NORMAL = "normal"  # –û–±—ã—á–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã - –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç  
    LOW = "low"        # –ù–µ–∞–∫—Ç–∏–≤–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã - –∫–∞–∂–¥—ã–µ 30 –º–∏–Ω—É—Ç


class TokenBatch:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –±–∞—Ç—á–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
    def __init__(self, addresses: List[str], priority: TokenPriority):
        self.addresses = addresses
        self.priority = priority
        self.last_update = 0
        self.update_count = 0
    
    def __len__(self):
        return len(self.addresses)
    
    def should_update(self, current_time: float) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å —ç—Ç–æ—Ç –±–∞—Ç—á"""
        intervals = {
            TokenPriority.HIGH: 30,      # 30 —Å–µ–∫—É–Ω–¥
            TokenPriority.NORMAL: 300,   # 5 –º–∏–Ω—É—Ç
            TokenPriority.LOW: 1800      # 30 –º–∏–Ω—É—Ç
        }
        
        interval = intervals.get(self.priority, 300)
        return (current_time - self.last_update) >= interval


# ========== –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï –î–õ–Ø –ë–ê–¢–ß-–ú–û–ù–ò–¢–û–†–ò–ù–ì–ê ==========

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –±–∞—Ç—á–µ–π –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º
_token_batches: Dict[TokenPriority, List[TokenBatch]] = {
    TokenPriority.HIGH: [],
    TokenPriority.NORMAL: [],
    TokenPriority.LOW: []
}

# –§–ª–∞–≥ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
_batch_monitoring_active = False

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
_monitoring_stats = {
    "total_updates": 0,
    "total_tokens_processed": 0,
    "last_update_time": 0,
    "errors_count": 0
}


# ========== –§–£–ù–ö–¶–ò–ò –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ü–†–ò–û–†–ò–¢–ï–¢–û–í ==========

def determine_token_priority(token_data: Dict[str, Any]) -> TokenPriority:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–æ–∫–µ–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.
    
    Args:
        token_data: –î–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∞ –∏–∑ –±–∞–∑—ã
        
    Returns:
        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–æ–∫–µ–Ω–∞
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        current_multiplier = token_data.get('current_multiplier', 1.0)
        last_alert_multiplier = token_data.get('last_alert_multiplier', 1.0)
        last_updated = token_data.get('last_updated', 0)
        
        current_time = time.time()
        time_since_update = current_time - last_updated
        
        # HIGH –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Ç–æ–∫–µ–Ω—ã —Å –Ω–µ–¥–∞–≤–Ω–∏–º —Ä–æ—Å—Ç–æ–º –∏–ª–∏ –≤—ã—Å–æ–∫–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é
        if (current_multiplier > last_alert_multiplier * 1.1 or  # –†–æ—Å—Ç –Ω–∞ 10%+
            current_multiplier >= 2.0 or                         # –ú—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–æ—Ä >= 2x
            time_since_update < 3600):                           # –û–±–Ω–æ–≤–ª—è–ª—Å—è –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å
            return TokenPriority.HIGH
        
        # LOW –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        elif (current_multiplier < 1.5 and                      # –ù–∏–∑–∫–∏–π —Ä–æ—Å—Ç
              time_since_update > 86400):                       # –ù–µ –æ–±–Ω–æ–≤–ª—è–ª—Å—è > 24 —á–∞—Å–æ–≤
            return TokenPriority.LOW
        
        # NORMAL –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ
        else:
            return TokenPriority.NORMAL
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ —Ç–æ–∫–µ–Ω–∞: {e}")
        return TokenPriority.NORMAL


def categorize_tokens_by_priority() -> Dict[TokenPriority, List[str]]:
    """
    –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º.
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å {–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: [—Å–ø–∏—Å–æ–∫_–∞–¥—Ä–µ—Å–æ–≤]}
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        all_tokens = token_storage.get_all_tokens()
        
        if not all_tokens:
            logger.warning("–ù–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏")
            return {priority: [] for priority in TokenPriority}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º
        prioritized_tokens = {priority: [] for priority in TokenPriority}
        
        for token_data in all_tokens:
            # –ï—Å–ª–∏ token_data - —Å—Ç—Ä–æ–∫–∞, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
            if isinstance(token_data, str):
                token_data = {'query': token_data, 'is_active': True}
    
            if not isinstance(token_data, dict):
                continue
        
            if not token_data.get('is_active', True):
                continue
        
            address = token_data.get('query', '')
            if not address:
                continue
        
            priority = determine_token_priority(token_data)
            prioritized_tokens[priority].append(address)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        for priority, addresses in prioritized_tokens.items():
            logger.info(f"–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç {priority.value}: {len(addresses)} —Ç–æ–∫–µ–Ω–æ–≤")
        
        return prioritized_tokens
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤: {e}")
        return {priority: [] for priority in TokenPriority}


# ========== –§–£–ù–ö–¶–ò–ò –°–û–ó–î–ê–ù–ò–Ø –ë–ê–¢–ß–ï–ô ==========

def create_token_batches(max_batch_size: int = 30) -> None:
    """
    –°–æ–∑–¥–∞–µ—Ç –±–∞—Ç—á–∏ —Ç–æ–∫–µ–Ω–æ–≤, —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º.
    
    Args:
        max_batch_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞
    """
    global _token_batches
    
    try:
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–µ–π —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º...")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã, —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º
        prioritized_tokens = categorize_tokens_by_priority()
        
        # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–∞—Ç—á–∏
        _token_batches = {priority: [] for priority in TokenPriority}
        
        # –°–æ–∑–¥–∞–µ–º –±–∞—Ç—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        for priority, addresses in prioritized_tokens.items():
            if not addresses:
                continue
                
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∞–¥—Ä–µ—Å–∞
            valid_addresses = validate_token_addresses(addresses)
            
            if not valid_addresses:
                logger.warning(f"–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ {priority.value}")
                continue
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
            address_batches = split_into_batches(valid_addresses, max_batch_size)
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç—ã TokenBatch
            for batch_addresses in address_batches:
                batch = TokenBatch(batch_addresses, priority)
                _token_batches[priority].append(batch)
            
            logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(address_batches)} –±–∞—Ç—á–µ–π –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ {priority.value}")
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_batches = sum(len(batches) for batches in _token_batches.values())
        total_tokens = sum(len(batch) for batches in _token_batches.values() for batch in batches)
        
        logger.info(f"–í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {total_batches} –±–∞—Ç—á–µ–π –¥–ª—è {total_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –±–∞—Ç—á–µ–π: {e}")
        import traceback
        logger.error(traceback.format_exc())


# ========== –§–£–ù–ö–¶–ò–ò –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê –ë–ê–¢–ß–ï–ô ==========

async def process_token_batch(batch: TokenBatch) -> None:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –±–∞—Ç—á —Ç–æ–∫–µ–Ω–æ–≤.
    
    Args:
        batch: –ë–∞—Ç—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    try:
        current_time = time.time()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å —ç—Ç–æ—Ç –±–∞—Ç—á
        if not batch.should_update(current_time):
            return
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ {batch.priority.value} —Å {len(batch)} —Ç–æ–∫–µ–Ω–∞–º–∏")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –±–∞—Ç—á–µ
        batch_results = fetch_tokens_batch(batch.addresses)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
        updated_count = 0
        error_count = 0
        
        for address, api_data in batch_results.items():
            try:
                # –í—ã–∑—ã–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
                # –ü–µ—Ä–µ–¥–∞–µ–º None –¥–ª—è context –∏ chat_id, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ —Ñ–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å
                token_info = await get_token_info(address, context=None, chat_id=None, 
                                                message_id=None)
                
                if token_info:
                    updated_count += 1
                else:
                    error_count += 1
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–æ–∫–µ–Ω–∞ {address}: {e}")
                error_count += 1
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞—Ç—á–∞
        batch.last_update = current_time
        batch.update_count += 1
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        _monitoring_stats["total_updates"] += 1
        _monitoring_stats["total_tokens_processed"] += updated_count
        _monitoring_stats["last_update_time"] = current_time
        _monitoring_stats["errors_count"] += error_count
        
        logger.info(f"–ë–∞—Ç—á –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {updated_count} —É—Å–ø–µ—à–Ω–æ, {error_count} –æ—à–∏–±–æ–∫")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞: {e}")
        import traceback
        logger.error(traceback.format_exc())


async def monitor_high_priority_tokens() -> None:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç —Ç–æ–∫–µ–Ω—ã –≤—ã—Å–æ–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ (–∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥)"""
    if not _batch_monitoring_active:
        return
        
    try:
        high_priority_batches = _token_batches.get(TokenPriority.HIGH, [])
        
        if not high_priority_batches:
            logger.debug("–ù–µ—Ç –±–∞—Ç—á–µ–π –≤—ã—Å–æ–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
            return
        
        logger.debug(f"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ {len(high_priority_batches)} HIGH –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –±–∞—Ç—á–µ–π")
        
        for batch in high_priority_batches:
            await process_token_batch(batch)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            await asyncio.sleep(1)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ HIGH –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞: {e}")


async def monitor_normal_priority_tokens() -> None:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç —Ç–æ–∫–µ–Ω—ã –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ (–∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç)"""
    if not _batch_monitoring_active:
        return
        
    try:
        normal_priority_batches = _token_batches.get(TokenPriority.NORMAL, [])
        
        if not normal_priority_batches:
            logger.debug("–ù–µ—Ç –±–∞—Ç—á–µ–π –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
            return
        
        logger.info(f"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ {len(normal_priority_batches)} NORMAL –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –±–∞—Ç—á–µ–π")
        
        for batch in normal_priority_batches:
            await process_token_batch(batch)
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            await asyncio.sleep(2)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ NORMAL –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞: {e}")


async def monitor_low_priority_tokens() -> None:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç —Ç–æ–∫–µ–Ω—ã –Ω–∏–∑–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ (–∫–∞–∂–¥—ã–µ 30 –º–∏–Ω—É—Ç)"""
    if not _batch_monitoring_active:
        return
        
    try:
        low_priority_batches = _token_batches.get(TokenPriority.LOW, [])
        
        if not low_priority_batches:
            logger.debug("–ù–µ—Ç –±–∞—Ç—á–µ–π –Ω–∏–∑–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")
            return
        
        logger.info(f"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ {len(low_priority_batches)} LOW –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –±–∞—Ç—á–µ–π")
        
        for batch in low_priority_batches:
            await process_token_batch(batch)
            
            # –ë–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ –Ω–∏–∑–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
            await asyncio.sleep(5)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ LOW –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞: {e}")


# ========== –§–£–ù–ö–¶–ò–ò –£–ü–†–ê–í–õ–ï–ù–ò–Ø –ú–û–ù–ò–¢–û–†–ò–ù–ì–û–ú ==========

async def start_batch_monitoring_system() -> None:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–∏—Å—Ç–µ–º—É –±–∞—Ç—á-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º task_scheduler"""
    global _batch_monitoring_active
    
    try:
        logger.info("–ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã –±–∞—Ç—á-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ç–æ–∫–µ–Ω–æ–≤...")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –±–∞—Ç—á–∏
        create_token_batches()
        
        # –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        _batch_monitoring_active = True
        
        # –ü–ª–∞–Ω–∏—Ä—É–µ–º –∑–∞–¥–∞—á–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏
        
        # HIGH –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
        scheduler.schedule_task(
            "monitor_high_priority",
            monitor_high_priority_tokens,
            delay=10,        # –ù–∞—á–∏–Ω–∞–µ–º —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥
            interval=30,     # –ü–æ–≤—Ç–æ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
            priority=TaskPriority.HIGH
        )
        
        # NORMAL –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
        scheduler.schedule_task(
            "monitor_normal_priority", 
            monitor_normal_priority_tokens,
            delay=60,        # –ù–∞—á–∏–Ω–∞–µ–º —á–µ—Ä–µ–∑ 1 –º–∏–Ω—É—Ç—É
            interval=300,    # –ü–æ–≤—Ç–æ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
            priority=TaskPriority.NORMAL
        )
        
        # LOW –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∫–∞–∂–¥—ã–µ 30 –º–∏–Ω—É—Ç
        scheduler.schedule_task(
            "monitor_low_priority",
            monitor_low_priority_tokens,
            delay=120,       # –ù–∞—á–∏–Ω–∞–µ–º —á–µ—Ä–µ–∑ 2 –º–∏–Ω—É—Ç—ã
            interval=1800,   # –ü–æ–≤—Ç–æ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 30 –º–∏–Ω—É—Ç
            priority=TaskPriority.LOW
        )
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–µ–π –∫–∞–∂–¥—ã–µ 10 –º–∏–Ω—É—Ç (–¥–ª—è –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤)
        scheduler.schedule_task(
            "recreate_batches",
            recreate_token_batches,
            delay=600,       # –ù–∞—á–∏–Ω–∞–µ–º —á–µ—Ä–µ–∑ 10 –º–∏–Ω—É—Ç
            interval=600,    # –ü–æ–≤—Ç–æ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 10 –º–∏–Ω—É—Ç
            priority=TaskPriority.NORMAL
        )
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
        scheduler.schedule_task(
            "log_monitoring_stats",
            log_monitoring_statistics,
            delay=300,       # –ù–∞—á–∏–Ω–∞–µ–º —á–µ—Ä–µ–∑ 5 –º–∏–Ω—É—Ç
            interval=300,    # –ü–æ–≤—Ç–æ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
            priority=TaskPriority.LOW
        )
        
        logger.info("–°–∏—Å—Ç–µ–º–∞ –±–∞—Ç—á-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞–ø—É—â–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–∏—Å—Ç–µ–º—ã –±–∞—Ç—á-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
        import traceback
        logger.error(traceback.format_exc())


async def stop_batch_monitoring_system() -> None:
    """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–∏—Å—Ç–µ–º—É –±–∞—Ç—á-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    global _batch_monitoring_active
    
    try:
        logger.info("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º—ã –±–∞—Ç—á-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞...")
        
        # –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        _batch_monitoring_active = False
        
        # –û—Ç–º–µ–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        monitoring_tasks = [
            "monitor_high_priority",
            "monitor_normal_priority", 
            "monitor_low_priority",
            "recreate_batches",
            "log_monitoring_stats"
        ]
        
        for task_id in monitoring_tasks:
            if scheduler.cancel_task(task_id):
                logger.info(f"–ó–∞–¥–∞—á–∞ {task_id} –æ—Ç–º–µ–Ω–µ–Ω–∞")
        
        # –û—á–∏—â–∞–µ–º –±–∞—Ç—á–∏
        global _token_batches
        _token_batches = {priority: [] for priority in TokenPriority}
        
        logger.info("–°–∏—Å—Ç–µ–º–∞ –±–∞—Ç—á-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ —Å–∏—Å—Ç–µ–º—ã –±–∞—Ç—á-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")


async def recreate_token_batches() -> None:
    """–ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç –±–∞—Ç—á–∏ —Ç–æ–∫–µ–Ω–æ–≤ (–¥–ª—è –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤)"""
    try:
        logger.info("–ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–µ–π —Ç–æ–∫–µ–Ω–æ–≤...")
        create_token_batches()
        logger.info("–ë–∞—Ç—á–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–∏ –±–∞—Ç—á–µ–π: {e}")


async def log_monitoring_statistics() -> None:
    """–õ–æ–≥–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    try:
        stats = get_monitoring_statistics()
        
        logger.info("=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ê–¢–ß-–ú–û–ù–ò–¢–û–†–ò–ù–ì–ê ===")
        logger.info(f"–í—Å–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π: {stats['total_updates']}")
        logger.info(f"–¢–æ–∫–µ–Ω–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['total_tokens_processed']}")
        logger.info(f"–û—à–∏–±–æ–∫: {stats['errors_count']}")
        logger.info(f"–ê–∫—Ç–∏–≤–Ω—ã—Ö –±–∞—Ç—á–µ–π: {stats['active_batches']}")
        logger.info(f"–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {stats['last_update_ago']:.1f} —Å–µ–∫ –Ω–∞–∑–∞–¥")
        logger.info("=====================================")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")


# ========== –§–£–ù–ö–¶–ò–ò –ü–û–õ–£–ß–ï–ù–ò–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ò ==========

def get_monitoring_statistics() -> Dict[str, Any]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –±–∞—Ç—á-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    current_time = time.time()
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –±–∞—Ç—á–∏
    active_batches = sum(len(batches) for batches in _token_batches.values())
    
    # –í—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    last_update_ago = current_time - _monitoring_stats.get("last_update_time", current_time)
    
    return {
        "active": _batch_monitoring_active,
        "total_updates": _monitoring_stats.get("total_updates", 0),
        "total_tokens_processed": _monitoring_stats.get("total_tokens_processed", 0),
        "errors_count": _monitoring_stats.get("errors_count", 0),
        "active_batches": active_batches,
        "last_update_time": _monitoring_stats.get("last_update_time", 0),
        "last_update_ago": last_update_ago,
        "batches_by_priority": {
            priority.value: len(batches) 
            for priority, batches in _token_batches.items()
        }
    }


def get_batch_details() -> Dict[str, Any]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –±–∞—Ç—á–∞—Ö"""
    details = {}
    
    for priority, batches in _token_batches.items():
        priority_details = []
        
        for i, batch in enumerate(batches):
            batch_info = {
                "batch_id": i,
                "size": len(batch),
                "last_update": batch.last_update,
                "update_count": batch.update_count,
                "addresses": batch.addresses[:3]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 –∞–¥—Ä–µ—Å–∞
            }
            priority_details.append(batch_info)
        
        details[priority.value] = {
            "count": len(batches),
            "total_tokens": sum(len(batch) for batch in batches),
            "batches": priority_details
        }
    
    return details


# ========== –§–£–ù–ö–¶–ò–ò –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –° –°–£–©–ï–°–¢–í–£–Æ–©–ò–ú –ö–û–î–û–ú ==========

async def start_token_monitoring_system(application):
    """
    –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞.
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –±–∞—Ç—á-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥.
    """
    try:
        logger.info("–ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ç–æ–∫–µ–Ω–æ–≤...")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞—á
        scheduler.start()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–∏—Å—Ç–µ–º—É –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        # ... –∑–¥–µ—Å—å –∫–æ–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ start_token_monitoring_system ...
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ù–û–í–£–Æ —Å–∏—Å—Ç–µ–º—É –±–∞—Ç—á-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        await start_batch_monitoring_system()
        
        logger.info("–°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞–ø—É—â–µ–Ω–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
        import traceback
        logger.error(traceback.format_exc())


# ========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ê–î–ú–ò–ù –ü–ê–ù–ï–õ–ò ==========

def format_monitoring_status() -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—É—Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∞–¥–º–∏–Ω –ø–∞–Ω–µ–ª–∏"""
    try:
        stats = get_monitoring_statistics()
        
        status = "üîÑ" if stats["active"] else "‚è∏Ô∏è"
        
        message = f"""üìä *–°—Ç–∞—Ç—É—Å –±–∞—Ç—á-–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞* {status}

üìà *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:*
‚Ä¢ –í—Å–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π: {stats['total_updates']}
‚Ä¢ –¢–æ–∫–µ–Ω–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['total_tokens_processed']}
‚Ä¢ –û—à–∏–±–æ–∫: {stats['errors_count']}
‚Ä¢ –ê–∫—Ç–∏–≤–Ω—ã—Ö –±–∞—Ç—á–µ–π: {stats['active_batches']}

üéØ *–ë–∞—Ç—á–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º:*
‚Ä¢ HIGH (30 —Å–µ–∫): {stats['batches_by_priority']['high']} –±–∞—Ç—á–µ–π
‚Ä¢ NORMAL (5 –º–∏–Ω): {stats['batches_by_priority']['normal']} –±–∞—Ç—á–µ–π  
‚Ä¢ LOW (30 –º–∏–Ω): {stats['batches_by_priority']['low']} –±–∞—Ç—á–µ–π

‚è∞ –ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {stats['last_update_ago']:.1f} —Å–µ–∫ –Ω–∞–∑–∞–¥"""

        return message
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å—Ç–∞—Ç—É—Å–∞: {e}")
        return "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç—É—Å–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"


# ========== –≠–ö–°–ü–û–†–¢ –ù–û–í–´–• –§–£–ù–ö–¶–ò–ô ==========

__all__ = [
    # –ö–ª–∞—Å—Å—ã
    'TokenPriority',
    'TokenBatch',
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    'start_batch_monitoring_system',
    'stop_batch_monitoring_system',
    'create_token_batches',
    
    # –§—É–Ω–∫—Ü–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    'get_monitoring_statistics',
    'get_batch_details',
    'format_monitoring_status',
    
    # –§—É–Ω–∫—Ü–∏–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
    'determine_token_priority',
    'categorize_tokens_by_priority'
]