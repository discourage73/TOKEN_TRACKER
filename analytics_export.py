import sqlite3
import pandas as pd
from datetime import datetime
import json
import logging
import os

logger = logging.getLogger(__name__)

def export_tokens_analytics() -> str:
    """
    –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ Excel —Ñ–∞–π–ª.
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ mcap_monitoring –∏ tokens —Ç–∞–±–ª–∏—Ü:
    - mcap_monitoring: market cap –¥–∞–Ω–Ω—ã–µ, –º–Ω–æ–∂–∏—Ç–µ–ª–∏, —Å—Ç–∞—Ç—É—Å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    - tokens: —Å–∏–≥–Ω–∞–ª—ã –∫–∞–Ω–∞–ª–æ–≤, –≤—Ä–µ–º–µ–Ω–∞, —Å—Ç–∞—Ç—É—Å –æ—Ç–ø—Ä–∞–≤–∫–∏
    
    Returns:
        str: –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É Excel —Ñ–∞–π–ª—É
    """
    try:
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º —ç–∫—Å–ø–æ—Ä—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤")
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        conn = sqlite3.connect("tokens_tracker_database.db")
        
        # SQL –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ–±–µ–∏—Ö —Ç–∞–±–ª–∏—Ü
        query = """
        SELECT 
            -- –î–∞–Ω–Ω—ã–µ –∏–∑ mcap_monitoring
            m.contract,
            m.initial_mcap,
            m.curr_mcap,
            m.ath_mcap,
            m.created_time as monitoring_start,
            m.updated_time as last_updated,
            m.ath_time,
            m.last_alert_multiplier,
            m.is_active,
            
            -- –î–∞–Ω–Ω—ã–µ –∏–∑ tokens (—Å–∏–≥–Ω–∞–ª—ã –∏ –∫–∞–Ω–∞–ª—ã)
            t.channels,
            t.channel_times,
            t.channel_count,
            t.first_seen,
            t.signal_reached_time,
            t.time_to_threshold,
            t.message_sent,
            t.message_id,
            t.token_info,
            t.raw_api_data
            
        FROM mcap_monitoring m
        LEFT JOIN tokens t ON m.contract = t.contract
        ORDER BY m.created_time DESC
        """
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        df = process_export_data(df)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ real_multiplier –æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É
        df = df.sort_values('real_multiplier', ascending=False, na_position='last')
        
        # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Å —Ç–µ–∫—É—â–µ–π –¥–∞—Ç–æ–π
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"tokens_analytics_{timestamp}.xlsx"
        filepath = os.path.join("exports", filename)
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É exports –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs("exports", exist_ok=True)
        
        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ Excel —Å —á–µ—Ç—ã—Ä—å–º—è –ª–∏—Å—Ç–∞–º–∏
        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
            # –û—Å–Ω–æ–≤–Ω–æ–π –ª–∏—Å—Ç —Å –¥–∞–Ω–Ω—ã–º–∏
            df.to_excel(writer, sheet_name='Tokens_Analytics', index=False)
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            main_stats_df, daily_stats_df = create_stats_summary_separate(df)
            main_stats_df.to_excel(writer, sheet_name='Statistics', index=False)
            
            # –î–Ω–µ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ª–∏—Å—Ç–µ —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å—Ç–æ–ª–±—Ü–∞–º–∏
            if not daily_stats_df.empty:
                daily_stats_df.to_excel(writer, sheet_name='Daily_Stats', index=False)
            
            # –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∫–∞–Ω–∞–ª–æ–≤
            channels_df = create_channels_analytics(df)
            if not channels_df.empty:
                channels_df.to_excel(writer, sheet_name='Channels', index=False)
            
            # Theory –∞–Ω–∞–ª–∏–∑
            theory_df = create_theory_analysis(df)
            if not theory_df.empty:
                theory_df.to_excel(writer, sheet_name='Theory', index=False)
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞–≤—Ç–æ—à–∏—Ä–∏–Ω—É —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –≤—Å–µ—Ö –ª–∏—Å—Ç–æ–≤
            for sheet_name in writer.sheets:
                worksheet = writer.sheets[sheet_name]
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    
                    adjusted_width = min(max_length + 2, 50)  # –ú–∞–∫—Å–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤
                    worksheet.column_dimensions[column_letter].width = adjusted_width
        
        logger.info(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {filepath}")
        return filepath
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {e}")
        raise

def process_export_data(df: pd.DataFrame) -> pd.DataFrame:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ –≤ Excel."""
    try:
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –º–Ω–æ–∂–∏—Ç–µ–ª—å
        df['real_multiplier'] = (df['ath_mcap'] / df['initial_mcap']).round(2)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º JSON –ø–æ–ª—è –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        df['token_name'] = df['token_info'].apply(extract_token_name)
        df['token_symbol'] = df['token_info'].apply(extract_token_symbol)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–Ω–∞–ª—ã
        df['channels_list'] = df['channels'].apply(parse_channels_json)
        df['signals_count'] = df['channel_count'].fillna(0)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—ã
        date_columns = ['monitoring_start', 'last_updated', 'ath_time', 'first_seen', 'signal_reached_time']
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—á–µ—Ç–Ω—ã–µ –ø–æ–ª—è
        df['days_in_monitoring'] = (datetime.now() - df['monitoring_start']).dt.days
        df['mcap_change_percent'] = ((df['curr_mcap'] - df['initial_mcap']) / df['initial_mcap'] * 100).round(2)
        
        # –ü–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ (—É–±–∏—Ä–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ)
        column_order = [
            'contract', 'token_name', 'is_active',
            'signals_count', 'initial_mcap', 'curr_mcap', 'ath_mcap',
            'real_multiplier', 'monitoring_start', 'first_seen', 
            'ath_time', 'time_to_threshold'
        ]
        
        # –°—Ç–æ–ª–±—Ü—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        columns_to_exclude = [
            'message_sent', 'token_symbol', 'last_alert_multiplier', 
            'mcap_change_percent', 'raw_api_data', 'token_info', 'message_id',
            'channels_list', 'signal_reached_time', 'days_in_monitoring', 'channel_count'
        ]
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        existing_columns = [col for col in column_order if col in df.columns]
        remaining_columns = [col for col in df.columns if col not in existing_columns and col not in columns_to_exclude]
        df = df[existing_columns + remaining_columns]
        
        return df
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return df

def extract_token_name(token_info_json):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è —Ç–æ–∫–µ–Ω–∞ –∏–∑ JSON."""
    try:
        if pd.isna(token_info_json) or not token_info_json:
            return "Unknown"
        data = json.loads(token_info_json)
        return data.get('name', data.get('ticker', 'Unknown'))
    except:
        return "Unknown"

def extract_token_symbol(token_info_json):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–∏–º–≤–æ–ª —Ç–æ–∫–µ–Ω–∞ –∏–∑ JSON."""
    try:
        if pd.isna(token_info_json) or not token_info_json:
            return "UNK"
        data = json.loads(token_info_json)
        return data.get('ticker', data.get('symbol', 'UNK'))
    except:
        return "UNK"

def parse_channels_json(channels_json):
    """–ü–∞—Ä—Å–∏—Ç JSON –∫–∞–Ω–∞–ª–æ–≤ –≤ —á–∏—Ç–∞–µ–º—É—é —Å—Ç—Ä–æ–∫—É."""
    try:
        if pd.isna(channels_json) or not channels_json:
            return ""
        channels = json.loads(channels_json)
        if isinstance(channels, list):
            return ", ".join(channels[:5])  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –∫–∞–Ω–∞–ª–æ–≤
        return str(channels)
    except:
        return "Error parsing channels"

def create_stats_summary(df: pd.DataFrame) -> pd.DataFrame:
    """–°–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
    # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_tokens = len(df)
    active_tokens = len(df[df['is_active'] == 1])
    rug_ratio = int(((total_tokens - active_tokens) / total_tokens * 100)) if total_tokens > 0 else 0
    
    stats = {
        'Metric': [
            'Total Tokens',
            'Active Tokens',
            'Tokens with Growth ‚â•2x',
            'Tokens with Growth ‚â•5x',
            'Tokens with Growth ‚â•10x',
            'Average Signals per Token',
            'High Growth Rate (‚â•2x/Total)',
            'RUG Ratio'
        ],
        'Value': [
            total_tokens,
            active_tokens,
            len(df[df['real_multiplier'] >= 2]),
            len(df[df['real_multiplier'] >= 5]),
            len(df[df['real_multiplier'] >= 10]),
            df['signals_count'].mean().round(1),
            f"{(len(df[df['real_multiplier'] >= 2]) / total_tokens * 100):.1f}%" if total_tokens > 0 else "0%",
            f"{rug_ratio}%"
        ]
    }
    
    main_stats_df = pd.DataFrame(stats)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    separator_df = pd.DataFrame({
        'Metric': ['', ''],
        'Value': ['', '']
    })
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–Ω—è–º
    daily_stats_df = create_daily_stats(df)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤ –æ–¥–∏–Ω DataFrame
    result_df = pd.concat([main_stats_df, separator_df, daily_stats_df], ignore_index=True)
    
    return result_df

def create_stats_summary_separate(df: pd.DataFrame):
    """–°–æ–∑–¥–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –¥–Ω–µ–≤–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
    # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_tokens = len(df)
    active_tokens = len(df[df['is_active'] == 1])
    rug_ratio = int(((total_tokens - active_tokens) / total_tokens * 100)) if total_tokens > 0 else 0
    
    main_stats = {
        'Metric': [
            'Total Tokens',
            'Active Tokens',
            'Tokens with Growth ‚â•2x',
            'Tokens with Growth ‚â•5x',
            'Tokens with Growth ‚â•10x',
            'Average Signals per Token',
            'High Growth Rate (‚â•2x/Total)',
            'RUG Ratio'
        ],
        'Value': [
            total_tokens,
            active_tokens,
            len(df[df['real_multiplier'] >= 2]),
            len(df[df['real_multiplier'] >= 5]),
            len(df[df['real_multiplier'] >= 10]),
            df['signals_count'].mean().round(1),
            f"{(len(df[df['real_multiplier'] >= 2]) / total_tokens * 100):.1f}%" if total_tokens > 0 else "0%",
            f"{rug_ratio}%"
        ]
    }
    
    main_stats_df = pd.DataFrame(main_stats)
    daily_stats_df = create_daily_stats_separate(df)
    
    return main_stats_df, daily_stats_df

def create_daily_stats_separate(df: pd.DataFrame) -> pd.DataFrame:
    """–°–æ–∑–¥–∞–µ—Ç –¥–Ω–µ–≤–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å—Ç–æ–ª–±—Ü–∞–º–∏."""
    try:
        from datetime import datetime, timedelta
        import pytz
        
        # –ú–æ—Å–∫–æ–≤—Å–∫–∏–π —á–∞—Å–æ–≤–æ–π –ø–æ—è—Å
        msk_tz = pytz.timezone('Europe/Moscow')
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–∞—Ç—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        df_copy = df.copy()
        df_copy['monitoring_start'] = pd.to_datetime(df_copy['monitoring_start'])
        
        if df_copy['monitoring_start'].empty:
            return pd.DataFrame()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
        min_date = df_copy['monitoring_start'].min().date()
        max_date = df_copy['monitoring_start'].max().date()
        
        daily_data = []
        current_date = min_date
        
        while current_date <= max_date:
            # –ü–µ—Ä–∏–æ–¥ —Å 14:00 —Ç–µ–∫—É—â–µ–≥–æ –¥–Ω—è –¥–æ 13:59 —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–Ω—è (–ú–°–ö)
            period_start = datetime.combine(current_date, datetime.min.time().replace(hour=14, minute=0))
            period_end = period_start + timedelta(days=1) - timedelta(minutes=1)  # 13:59 —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–Ω—è
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ UTC –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            period_start_utc = msk_tz.localize(period_start).astimezone(pytz.UTC).replace(tzinfo=None)
            period_end_utc = msk_tz.localize(period_end).astimezone(pytz.UTC).replace(tzinfo=None)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –∑–∞ —ç—Ç–æ—Ç –ø–µ—Ä–∏–æ–¥
            day_tokens = df_copy[
                (df_copy['monitoring_start'] >= period_start_utc) & 
                (df_copy['monitoring_start'] <= period_end_utc)
            ]
            
            if len(day_tokens) > 0:
                total_day_tokens = len(day_tokens)
                day_growth_2x = len(day_tokens[day_tokens['real_multiplier'] >= 2])
                day_active = len(day_tokens[day_tokens['is_active'] == 1])
                day_rug_ratio = int(((total_day_tokens - day_active) / total_day_tokens * 100)) if total_day_tokens > 0 else 0
                day_growth_rate = round((day_growth_2x / total_day_tokens * 100), 1) if total_day_tokens > 0 else 0
                
                daily_data.append({
                    'Day': current_date.strftime('%Y-%m-%d'),
                    'Time_Period': '14:00-13:59 MSK',
                    'Total_Tokens': total_day_tokens,
                    'Tokens_Growth_2x': day_growth_2x,
                    'RUG_Ratio_Percent': day_rug_ratio,
                    'High_Growth_Rate_Percent': day_growth_rate
                })
            
            current_date += timedelta(days=1)
        
        if not daily_data:
            return pd.DataFrame()
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –¥–Ω–µ–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        daily_df = pd.DataFrame(daily_data)
        
        return daily_df
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–Ω–µ–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        return pd.DataFrame()

def create_channels_analytics(df: pd.DataFrame) -> pd.DataFrame:
    """–°–æ–∑–¥–∞–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫—É –∫–∞–Ω–∞–ª–æ–≤."""
    try:
        import json
        from collections import defaultdict
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞–Ω–∞–ª–∞–º
        channel_stats = defaultdict(lambda: {
            'total_signals': 0,
            'successful_signals': 0, 
            'tokens': [],
            'entry_positions': []  # –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ –º–µ—Å—Ç–∞ –≤—Ö–æ–¥–∞
        })
        
        logger.info("üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–Ω–∞–ª—ã...")
        
        for _, token in df.iterrows():
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–∞–ª—ã —Ç–æ–∫–µ–Ω–∞
                channels_data = token.get('channels')
                if pd.isna(channels_data) or not channels_data or channels_data.strip() == '':
                    continue
                
                # –ü–∞—Ä—Å–∏–º –∫–∞–Ω–∞–ª—ã - —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∫–∞–∫ JSON, –ø–æ—Ç–æ–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É —Å –∑–∞–ø—è—Ç—ã–º–∏
                channels = []
                try:
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON –º–∞—Å—Å–∏–≤
                    channels = json.loads(channels_data)
                    if not isinstance(channels, list):
                        channels = []
                except (json.JSONDecodeError, TypeError):
                    # –ï—Å–ª–∏ –Ω–µ JSON, –ø–∞—Ä—Å–∏–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
                    if isinstance(channels_data, str):
                        channels = [ch.strip() for ch in channels_data.split(',') if ch.strip()]
                
                if not channels:
                    continue
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∞
                real_multiplier = token.get('real_multiplier', 0)
                is_successful = real_multiplier >= 2
                
                # –ü–æ–ª—É—á–∞–µ–º –≤—Ä–µ–º–µ–Ω–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∫–∞–Ω–∞–ª–æ–≤
                channel_times_json = token.get('channel_times', '{}')
                try:
                    if pd.isna(channel_times_json) or not channel_times_json or channel_times_json.strip() == '':
                        channel_times = {}
                    else:
                        channel_times = json.loads(channel_times_json)
                        if not isinstance(channel_times, dict):
                            channel_times = {}
                except (json.JSONDecodeError, TypeError):
                    channel_times = {}
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞–Ω–∞–ª—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—Ö–æ–∂–¥–µ–Ω–∏—è (–∫—Ç–æ –ø–µ—Ä–≤—ã–π –≤–æ—à–µ–ª)
                channels_with_times = []
                for channel in channels:
                    entry_time = channel_times.get(channel, '')
                    channels_with_times.append((channel, entry_time))
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (—Ä–∞–Ω–Ω–∏–µ —Å–∏–≥–Ω–∞–ª—ã –ø–µ—Ä–≤—ã–µ)
                channels_with_times.sort(key=lambda x: x[1] if x[1] else '9999-12-31')
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞
                for position, (channel, entry_time) in enumerate(channels_with_times, 1):
                    channel_stats[channel]['total_signals'] += 1
                    channel_stats[channel]['entry_positions'].append(position)
                    
                    if is_successful:
                        channel_stats[channel]['successful_signals'] += 1
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω –≤ —Å–ø–∏—Å–æ–∫
                    token_info = {
                        'contract': token.get('contract', 'Unknown')[:12] + '...',
                        'token_name': token.get('token_name', 'Unknown'),
                        'multiplier': real_multiplier,
                        'successful': is_successful,
                        'entry_position': position,
                        'entry_time': entry_time
                    }
                    channel_stats[channel]['tokens'].append(token_info)
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–æ–∫–µ–Ω–∞ {token.get('contract', 'Unknown')}: {e}")
                continue
        
        if not channel_stats:
            return pd.DataFrame()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        channels_data = []
        
        for channel_name, stats in channel_stats.items():
            total_signals = stats['total_signals']
            successful_signals = stats['successful_signals']
            success_rate = (successful_signals / total_signals * 100) if total_signals > 0 else 0
            
            # –°—Ä–µ–¥–Ω—è—è –ø–æ–∑–∏—Ü–∏—è –≤—Ö–æ–¥–∞
            avg_entry_position = sum(stats['entry_positions']) / len(stats['entry_positions']) if stats['entry_positions'] else 0
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ (—É—Å–ø–µ—à–Ω—ã–µ –ø–µ—Ä–≤—ã–µ, –ø–æ—Ç–æ–º –ø–æ –º–Ω–æ–∂–∏—Ç–µ–ª—é)
            tokens_sorted = sorted(stats['tokens'], key=lambda x: (-x['successful'], -x['multiplier']))
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É —Å–æ —Å–ø–∏—Å–∫–æ–º —Ç–æ–∫–µ–Ω–æ–≤
            tokens_list = []
            for token in tokens_sorted[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 —Ç–æ–∫–µ–Ω–æ–≤
                status = "‚úÖ" if token['successful'] else "‚ùå"
                tokens_list.append(f"{status} {token['token_name']} ({token['multiplier']:.1f}x, pos.{token['entry_position']})")
            
            tokens_string = " | ".join(tokens_list)
            if len(tokens_sorted) > 10:
                tokens_string += f" | ...and {len(tokens_sorted) - 10} more"
            
            channels_data.append({
                'Channel_Name': channel_name,
                'Total_Signals': total_signals,
                'Successful_Signals': successful_signals,
                'Success_Rate_Percent': round(success_rate, 1),
                'Average_Entry_Position': round(avg_entry_position, 1),
                'Top_Tokens': tokens_string
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞–Ω–∞–ª—ã –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏, –ø–æ—Ç–æ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–∏–≥–Ω–∞–ª–æ–≤
        channels_data.sort(key=lambda x: (-x['Success_Rate_Percent'], -x['Total_Signals']))
        
        channels_df = pd.DataFrame(channels_data)
        logger.info(f"üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(channels_df)} –∫–∞–Ω–∞–ª–æ–≤")
        
        return channels_df
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∫–∞–Ω–∞–ª–æ–≤: {e}")
        return pd.DataFrame()

def create_daily_stats(df: pd.DataFrame) -> pd.DataFrame:
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–Ω—è–º (—Å 14:00 –¥–æ 13:59 —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–Ω—è –ø–æ –ú–°–ö)."""
    try:
        from datetime import datetime, timedelta
        import pytz
        
        # –ú–æ—Å–∫–æ–≤—Å–∫–∏–π —á–∞—Å–æ–≤–æ–π –ø–æ—è—Å
        msk_tz = pytz.timezone('Europe/Moscow')
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–∞—Ç—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        df_copy = df.copy()
        df_copy['monitoring_start'] = pd.to_datetime(df_copy['monitoring_start'])
        
        if df_copy['monitoring_start'].empty:
            return pd.DataFrame({
                'Metric': ['Daily Statistics (14:00-13:59 MSK)'],
                'Value': ['No data available']
            })
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
        min_date = df_copy['monitoring_start'].min().date()
        max_date = df_copy['monitoring_start'].max().date()
        
        daily_data = []
        current_date = min_date
        
        while current_date <= max_date:
            # –ü–µ—Ä–∏–æ–¥ —Å 14:00 —Ç–µ–∫—É—â–µ–≥–æ –¥–Ω—è –¥–æ 13:59 —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–Ω—è (–ú–°–ö)
            period_start = datetime.combine(current_date, datetime.min.time().replace(hour=14, minute=0))
            period_end = period_start + timedelta(days=1) - timedelta(minutes=1)  # 13:59 —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–Ω—è
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ UTC –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –≤ UTC)
            period_start_utc = msk_tz.localize(period_start).astimezone(pytz.UTC).replace(tzinfo=None)
            period_end_utc = msk_tz.localize(period_end).astimezone(pytz.UTC).replace(tzinfo=None)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –∑–∞ —ç—Ç–æ—Ç –ø–µ—Ä–∏–æ–¥
            day_tokens = df_copy[
                (df_copy['monitoring_start'] >= period_start_utc) & 
                (df_copy['monitoring_start'] <= period_end_utc)
            ]
            
            if len(day_tokens) > 0:
                total_day_tokens = len(day_tokens)
                day_growth_2x = len(day_tokens[day_tokens['real_multiplier'] >= 2])
                day_active = len(day_tokens[day_tokens['is_active'] == 1])
                day_rug_ratio = int(((total_day_tokens - day_active) / total_day_tokens * 100)) if total_day_tokens > 0 else 0
                day_growth_rate = f"{(day_growth_2x / total_day_tokens * 100):.1f}%" if total_day_tokens > 0 else "0%"
                
                daily_data.append({
                    'Day': current_date.strftime('%Y-%m-%d'),
                    'Time Period': f"14:00-13:59 MSK",
                    'Total Tokens': total_day_tokens,
                    'Tokens ‚â•2x': day_growth_2x,
                    'RUG Ratio': f"{day_rug_ratio}%",
                    'Growth Rate': day_growth_rate
                })
            
            current_date += timedelta(days=1)
        
        if not daily_data:
            return pd.DataFrame({
                'Metric': ['Daily Statistics'],
                'Value': ['No daily data available']
            })
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –ø–æ –¥–Ω—è–º
        daily_df = pd.DataFrame(daily_data)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç Metric/Value –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è
        daily_stats_formatted = []
        daily_stats_formatted.append({'Metric': 'Daily Statistics (14:00-13:59 MSK)', 'Value': ''})
        daily_stats_formatted.append({'Metric': 'Day | Time | Total | ‚â•2x | RUG% | Growth%', 'Value': ''})
        
        for _, row in daily_df.iterrows():
            daily_stats_formatted.append({
                'Metric': f"{row['Day']} | {row['Time Period']} | {row['Total Tokens']} | {row['Tokens ‚â•2x']} | {row['RUG Ratio']} | {row['Growth Rate']}",
                'Value': ''
            })
        
        return pd.DataFrame(daily_stats_formatted)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–Ω–µ–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        return pd.DataFrame({
            'Metric': ['Daily Statistics Error'],
            'Value': [f'Error: {str(e)}']
        })

def create_theory_analysis(df: pd.DataFrame) -> pd.DataFrame:
    """–°–æ–∑–¥–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ —Ç–µ–æ—Ä–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —Å—Ç–∞—Ä—Ç–æ–≤–æ–π –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏."""
    try:
        logger.info("üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–æ—Ä–∏—é —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ –º–∞—Ä–∫–µ—Ç –∫–∞–ø...")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º market cap –≤ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        df_copy = df.copy()
        df_copy['initial_mcap'] = pd.to_numeric(df_copy['initial_mcap'], errors='coerce')
        df_copy['ath_mcap'] = pd.to_numeric(df_copy['ath_mcap'], errors='coerce')
        df_copy['curr_mcap'] = pd.to_numeric(df_copy['curr_mcap'], errors='coerce')
        
        # –£–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω—ã —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        df_clean = df_copy.dropna(subset=['initial_mcap', 'ath_mcap'])
        df_clean = df_clean[df_clean['initial_mcap'] > 0]
        
        theory_data = []
        
        # –¢–ï–û–†–ò–Ø 1: –¢–æ–∫–µ–Ω—ã, —Å—Ç–∞—Ä—Ç–æ–≤–∞–≤—à–∏–µ —Å –º–∞—Ä–∫–µ—Ç –∫–∞–ø < 1M
        theory1_tokens = df_clean[df_clean['initial_mcap'] < 1000000]
        theory1_total = len(theory1_tokens)
        
        if theory1_total > 0:
            # –°–∫–æ–ª—å–∫–æ –¥–æ—Å—Ç–∏–≥–ª–∏ 1M
            theory1_reached_1m = len(theory1_tokens[theory1_tokens['ath_mcap'] >= 1000000])
            theory1_1m_percent = round((theory1_reached_1m / theory1_total) * 100, 1)
            
            # –ò–∑ —Ç–µ—Ö, —á—Ç–æ –¥–æ—Å—Ç–∏–≥–ª–∏ 1M, —Å–∫–æ–ª—å–∫–æ –¥–æ—Å—Ç–∏–≥–ª–∏ 2M
            theory1_1m_tokens = theory1_tokens[theory1_tokens['ath_mcap'] >= 1000000]
            theory1_reached_2m = len(theory1_1m_tokens[theory1_1m_tokens['ath_mcap'] >= 2000000])
            theory1_2m_percent = round((theory1_reached_2m / theory1_reached_1m) * 100, 1) if theory1_reached_1m > 0 else 0
        else:
            theory1_reached_1m = 0
            theory1_1m_percent = 0
            theory1_2m_percent = 0
        
        # –¢–ï–û–†–ò–Ø 2: –¢–æ–∫–µ–Ω—ã, —Å—Ç–∞—Ä—Ç–æ–≤–∞–≤—à–∏–µ —Å –º–∞—Ä–∫–µ—Ç –∫–∞–ø > 1M
        theory2_tokens = df_clean[df_clean['initial_mcap'] > 1000000]
        theory2_total = len(theory2_tokens)
        
        if theory2_total > 0:
            # –°–∫–æ–ª—å–∫–æ –¥–æ—Å—Ç–∏–≥–ª–∏ x2 –∏ –±–æ–ª–µ–µ –æ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–π –∫–∞–ø—ã
            theory2_reached_2x = len(theory2_tokens[theory2_tokens['ath_mcap'] >= (theory2_tokens['initial_mcap'] * 2)])
            theory2_2x_percent = round((theory2_reached_2x / theory2_total) * 100, 1)
        else:
            theory2_reached_2x = 0
            theory2_2x_percent = 0
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        theory_data = [
            {
                'Category': 'THEORY 1: Tokens Starting < 1M',
                'Metric': 'Total Tokens',
                'Count': theory1_total,
                'Percentage': '100%'
            },
            {
                'Category': 'THEORY 1: Tokens Starting < 1M',
                'Metric': 'Reached 1M Market Cap',
                'Count': theory1_reached_1m,
                'Percentage': f'{theory1_1m_percent}%'
            },
            {
                'Category': 'THEORY 1: Tokens Starting < 1M',
                'Metric': 'After reaching 1M, reached 2M',
                'Count': theory1_reached_2m,
                'Percentage': f'{theory1_2m_percent}%'
            },
            {
                'Category': '',
                'Metric': '',
                'Count': '',
                'Percentage': ''
            },
            {
                'Category': 'THEORY 2: Tokens Starting > 1M',
                'Metric': 'Total Tokens',
                'Count': theory2_total,
                'Percentage': '100%'
            },
            {
                'Category': 'THEORY 2: Tokens Starting > 1M',
                'Metric': 'Reached x2+ Multiplier',
                'Count': theory2_reached_2x,
                'Percentage': f'{theory2_2x_percent}%'
            },
            {
                'Category': '',
                'Metric': '',
                'Count': '',
                'Percentage': ''
            },
            {
                'Category': 'SUMMARY',
                'Metric': 'Theory 1 Success Rate (< 1M ‚Üí 1M)',
                'Count': f'{theory1_reached_1m}/{theory1_total}',
                'Percentage': f'{theory1_1m_percent}%'
            },
            {
                'Category': 'SUMMARY',
                'Metric': 'Theory 1 Advanced Rate (1M ‚Üí 2M)',
                'Count': f'{theory1_reached_2m}/{theory1_reached_1m}' if theory1_reached_1m > 0 else '0/0',
                'Percentage': f'{theory1_2m_percent}%'
            },
            {
                'Category': 'SUMMARY',
                'Metric': 'Theory 2 Success Rate (> 1M ‚Üí x2)',
                'Count': f'{theory2_reached_2x}/{theory2_total}',
                'Percentage': f'{theory2_2x_percent}%'
            }
        ]
        
        theory_df = pd.DataFrame(theory_data)
        logger.info(f"üìä Theory –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: Theory1 ({theory1_total} tokens), Theory2 ({theory2_total} tokens)")
        
        return theory_df
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ theory –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return pd.DataFrame({
            'Category': ['Error'],
            'Metric': ['Theory Analysis Failed'],
            'Count': [str(e)],
            'Percentage': ['N/A']
        })

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ bot_commands.py
def handle_analytics_export():
    """–°–æ–∑–¥–∞–µ—Ç Excel —Ñ–∞–π–ª –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –Ω–µ–º—É."""
    try:
        filepath = export_tokens_analytics()
        return filepath
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {e}")
        raise

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    logging.basicConfig(level=logging.INFO)
    try:
        filepath = export_tokens_analytics()
        print(f"–§–∞–π–ª —Å–æ–∑–¥–∞–Ω: {filepath}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")